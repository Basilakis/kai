/**
 * Crawler Queue System
 * 
 * Provides a persistent, file-based queue system for crawler jobs,
 * supporting priorities, concurrency control, progress tracking,
 * error recovery, and integration with the training system.
 */

import * as fs from 'fs';
import * as path from 'path';
import { v4 as uuidv4 } from 'uuid';
import { logger } from '../../utils/logger';
import { 
  CrawlerConfig, 
  CrawlerJobOptions, 
  CrawlerResult,
  CrawlerService
} from './crawlerService.interface';
import { trainingProgressService } from '../training/progress-service';
import { crawlerServiceFactory, CrawlerProvider } from './crawlerServiceFactory';
import { createCrawlerQueueAdapter, QueueAdapter } from '../messaging/queueAdapter';
import { MessagePayload } from '../messaging/messageBroker';

/**
 * Status type for crawler jobs - matched to align with pdfQueue
 */
export type CrawlerJobStatus = 'pending' | 'processing' | 'completed' | 'failed' | 'retrying' | 'canceled' | 'training';

/**
 * Priority type for crawler jobs - matched to align with pdfQueue
 */
export type CrawlerJobPriority = 'low' | 'normal' | 'high';

/**
 * Structure for a crawler job
 */
export interface CrawlerJob {
  id: string;
  configId: string;
  config: CrawlerConfig;
  status: CrawlerJobStatus;
  provider: CrawlerProvider;
  priority: CrawlerJobPriority;
  createdAt: number; // renamed from addedAt to match pdfQueue
  startedAt?: number;
  completedAt?: number;
  progress?: number;
  error?: string;
  externalJobId?: string;
  result?: string; // Path to the stored result file
  trainingDataset?: string; // Path to the transformed training data
  trainingJobId?: string; // ID of any associated training job
  attempts: number; // matches pdfQueue's attempts tracking
  maxAttempts: number; // matches pdfQueue's maxAttempts tracking
}

/**
 * Options for adding a job to the queue
 */
export interface QueueJobOptions extends CrawlerJobOptions {
  priority?: CrawlerJobPriority; // Updated to string-based priority
  autoTrain?: boolean;
  trainingConfig?: {
    modelType?: 'hybrid' | 'feature-based' | 'ml-based';
    epochs?: number;
    batchSize?: number;
    learningRate?: number;
  };
}

/**
 * Crawler Queue class for managing crawler jobs
 */
export class CrawlerQueue {
  private jobs: Map<string, CrawlerJob> = new Map();
  private runningJobs: Set<string> = new Set();
  private queueFile: string;
  private resultsDir: string;
  private maxConcurrentJobs: number;
  private isProcessing: boolean = false;
  private autosaveInterval: NodeJS.Timeout | null = null;
  private maxAttempts: number = 3; // Default max attempts
  private adapter: QueueAdapter;

  /**
   * Create a new crawler queue
   * @param options Queue options
   */
  constructor(options: {
    queueFile?: string;
    resultsDir?: string;
    maxConcurrentJobs?: number;
    autosaveInterval?: number;
    maxAttempts?: number;
  } = {}) {
    // Initialize the queue adapter
    this.adapter = createCrawlerQueueAdapter();
    
    this.queueFile = options.queueFile || path.join(process.cwd(), 'data', 'crawler-queue.json');
    this.resultsDir = options.resultsDir || path.join(process.cwd(), 'data', 'crawler-results');
    this.maxConcurrentJobs = options.maxConcurrentJobs || 2;
    this.maxAttempts = options.maxAttempts || 3;

    // Create necessary directories
    fs.mkdirSync(path.dirname(this.queueFile), { recursive: true });
    fs.mkdirSync(this.resultsDir, { recursive: true });

    // Set up autosave interval
    if (options.autosaveInterval !== 0) {
      const interval = options.autosaveInterval || 60000; // Default: 1 minute
      this.autosaveInterval = setInterval(() => this.saveQueue(), interval);
    }

    // Restore queue state if available
    this.restoreQueue();
    
    // Set up event handlers for cross-queue events
    this.setupEventHandlers();
  }
  
  /**
   * Set up event handlers for events from other queues
   */
  private async setupEventHandlers(): Promise<void> {
    try {
      await this.adapter.setupStandardEventHandlers({
        // Handle job completion events from PDF queue
        onJobCompleted: async (message: MessagePayload) => {
          // Example: log cross-queue events
          logger.info(`Crawler Queue received completion event from ${message.source}: Job ${message.data.id}`);
          
          // Here you could implement logic to process PDFs that were generated by the crawler
          // For example, triggering a training job when a PDF has been processed
        },
        
        // Handle failure events from PDF queue
        onJobFailed: async (message: MessagePayload) => {
          logger.info(`Crawler Queue received failure event from ${message.source}: Job ${message.data.id}`);
        }
      });
      
      logger.info('Crawler Queue event handlers initialized');
    } catch (err) {
      logger.error(`Failed to set up Crawler Queue event handlers: ${err}`);
    }
  }

  /**
   * Convert string priority to number for internal sorting
   */
  private getPriorityValue(priority: CrawlerJobPriority): number {
    switch (priority) {
      case 'high': return 2;
      case 'normal': return 1;
      case 'low':
      default: return 0;
    }
  }

  /**
   * Save the queue state to disk
   */
  private saveQueue(): void {
    try {
      const serializedJobs = Array.from(this.jobs.values());
      fs.writeFileSync(this.queueFile, JSON.stringify(serializedJobs, null, 2));
      logger.debug(`Persisted crawler queue state: ${this.jobs.size} jobs`);
    } catch (err) {
      logger.error(`Failed to persist crawler queue state: ${err}`);
    }
  }

  /**
   * Restore the queue state from disk
   */
  private restoreQueue(): void {
    try {
      if (fs.existsSync(this.queueFile)) {
        const data = fs.readFileSync(this.queueFile, 'utf-8');
        const jobs = JSON.parse(data) as CrawlerJob[];
        
        this.jobs.clear();
        for (const job of jobs) {
          // Convert old priorities if needed
          if (typeof job.priority === 'number') {
            const numPriority = job.priority as unknown as number;
            if (numPriority >= 2) {
              job.priority = 'high';
            } else if (numPriority === 1) {
              job.priority = 'normal';
            } else {
              job.priority = 'low';
            }
          }
          
          // Convert old addedAt to createdAt if needed
          if ('addedAt' in job && !('createdAt' in job)) {
            (job as CrawlerJob).createdAt = (job as any).addedAt;
            delete (job as any).addedAt;
          }
          
          // Ensure attempts are tracked
          if (!('attempts' in job)) {
            (job as CrawlerJob).attempts = 0;
          }
          
          if (!('maxAttempts' in job)) {
            (job as CrawlerJob).maxAttempts = this.maxAttempts;
          }
          
          // Reset processing jobs on startup - handle legacy 'running' status
          if (job.status === 'processing' || (job as any).status === 'running') {
            job.status = 'retrying';
            job.error = 'Job was reset due to server restart';
          }
          
          this.jobs.set(job.id, job);
        }
        
        logger.info(`Restored crawler queue state: ${this.jobs.size} jobs`);
      }
    } catch (err) {
      logger.error(`Failed to restore crawler queue state: ${err}`);
    }
  }

  /**
   * Stop the queue and clean up resources
   */
  public stop(): void {
    if (this.autosaveInterval) {
      clearInterval(this.autosaveInterval);
      this.autosaveInterval = null;
    }
    
    // Unsubscribe from all message broker events
    this.adapter.unsubscribeAll().catch(err => {
      logger.error(`Failed to unsubscribe from message broker events: ${err}`);
    });
    
    this.saveQueue();
    logger.info('Crawler queue stopped');
  }

  /**
   * Add a crawler job to the queue
   * @param config Crawler configuration
   * @param options Job options
   * @returns Job ID
   */
  public async addJob(config: CrawlerConfig, options: QueueJobOptions = {}): Promise<string> {
    // Validate the config with the appropriate service
    try {
      const service = await crawlerServiceFactory.getServiceForConfig(config);
      const validation = await service.validateConfig(config);
      
      if (!validation.valid) {
        throw new Error(`Invalid crawler configuration: ${validation.errors?.join(', ')}`);
      }
      
      const jobId = uuidv4();
      const now = Date.now();
      
      const job: CrawlerJob = {
        id: jobId,
        configId: config.name,
        config,
        status: 'pending',
        provider: config.provider,
        priority: options.priority || 'normal',
        createdAt: now,
        attempts: 0,
        maxAttempts: this.maxAttempts
      };
      
      this.jobs.set(jobId, job);
      this.saveQueue();
      
      logger.info(`Added crawler job to queue: ${config.url} (Job ID: ${jobId})`);
      
      // Publish job-added event to the message broker
      await this.adapter.publishJobAdded({
        id: jobId,
        status: 'pending',
        priority: job.priority,
        url: config.url,
        provider: config.provider,
        configId: config.name
      });
      
      // Start processing if not already running
      if (!this.isProcessing) {
        this.processQueue();
      }
      
      return jobId;
    } catch (err) {
      logger.error(`Failed to add crawler job to queue: ${err}`);
      throw err;
    }
  }

  /**
   * Remove a job from the queue
   * @param jobId ID of the job to remove
   * @returns True if successful
   */
  public async removeJob(jobId: string): Promise<boolean> {
    const job = this.jobs.get(jobId);
    
    if (!job) {
      logger.warn(`Attempted to remove unknown job: ${jobId}`);
      return false;
    }
    
    if (job.status === 'processing') {
      // Stop the running job if possible
      try {
        const service = await crawlerServiceFactory.getServiceForConfig(job.config);
        if (job.externalJobId) {
          await service.stopJob(job.externalJobId);
        }
      } catch (err) {
        logger.error(`Failed to stop processing job ${jobId}: ${err}`);
      }
      
      this.runningJobs.delete(jobId);
    }
    
    this.jobs.delete(jobId);
    this.saveQueue();
    
    logger.info(`Removed job from queue: ${jobId}`);
    return true;
  }

  /**
   * Get a job by ID
   * @param jobId Job ID
   * @returns Job or undefined if not found
   */
  public getJob(jobId: string): CrawlerJob | undefined {
    return this.jobs.get(jobId);
  }

  /**
   * Get all jobs in the queue, optionally filtered by status
   * @param status Optional status filter
   * @returns Array of jobs
   */
  public getAll(status?: CrawlerJobStatus): CrawlerJob[] {
    const jobs = Array.from(this.jobs.values());
    
    if (status) {
      return jobs.filter(job => job.status === status);
    }
    
    return jobs;
  }

  /**
   * Get jobs by status
   * @param status Job status to filter by
   * @returns Array of matching jobs
   * @deprecated Use getAll(status) instead for consistency with pdfQueue
   */
  public getJobsByStatus(status: CrawlerJobStatus): CrawlerJob[] {
    return this.getAll(status);
  }

  /**
   * Legacy method for backward compatibility
   * @returns Map of job IDs to jobs
   * @deprecated Use getAll() instead for consistency with pdfQueue
   */
  public getAllJobs(): Map<string, CrawlerJob> {
    return new Map(this.jobs);
  }

  /**
   * Process the queue
   */
  private async processQueue(): Promise<void> {
    if (this.isProcessing) {
      return;
    }
    
    this.isProcessing = true;
    
    try {
      while (this.runningJobs.size < this.maxConcurrentJobs) {
        const nextJob = this.getNextJob();
        
        if (!nextJob) {
          break; // No more jobs to process
        }
        
        // Start processing the job
        this.runningJobs.add(nextJob.id);
        nextJob.status = 'processing'; // Changed from 'running' to 'processing'
        nextJob.startedAt = Date.now();
        nextJob.attempts += 1;
        this.saveQueue();
        
        // Process job in the background
        this.processJob(nextJob).finally(() => {
          this.runningJobs.delete(nextJob.id);
          
          // Continue processing the queue
          if (this.runningJobs.size < this.maxConcurrentJobs) {
            this.processQueue();
          }
        });
        
        // If we've reached the concurrency limit, break
        if (this.runningJobs.size >= this.maxConcurrentJobs) {
          break;
        }
      }
    } finally {
      this.isProcessing = false;
    }
  }

  /**
   * Get the next job to process
   * @returns Next job or undefined if no more jobs
   */
  private getNextJob(): CrawlerJob | undefined {
    // Get all pending or retrying jobs
    const pendingJobs = Array.from(this.jobs.values()).filter(
      job => job.status === 'pending' || job.status === 'retrying'
    );
    
    if (pendingJobs.length === 0) {
      return undefined;
    }
    
    // Sort by priority (higher first) and then by age (older first)
    pendingJobs.sort((a, b) => {
      const aPriority = this.getPriorityValue(a.priority);
      const bPriority = this.getPriorityValue(b.priority);
      
      if (aPriority !== bPriority) {
        return bPriority - aPriority; // Higher priority first
      }
      return a.createdAt - b.createdAt; // Older first
    });
    
    return pendingJobs[0];
  }

  /**
   * Process a crawler job
   * @param job Job to process
   */
  private async processJob(job: CrawlerJob): Promise<void> {
    logger.info(`Processing crawler job ${job.id} (${job.provider}): ${job.config.url} (Attempt ${job.attempts}/${job.maxAttempts})`);
    
    // Publish job-started event
    await this.adapter.publishJobStarted({
      id: job.id,
      status: 'processing',
      priority: job.priority,
      url: job.config.url,
      provider: job.provider,
      attempts: job.attempts,
      maxAttempts: job.maxAttempts
    });
    
    try {
      // Get the service for this job
      const service = await crawlerServiceFactory.getServiceForConfig(job.config);
      
      // Convert priority back to number for API calls that expect it
      const numericPriority = this.getPriorityValue(job.priority);
      
      // Start the job with string priority (service handles conversion internally)
      const startResult = await service.startJob(job.config, {
        priority: job.priority
      });
      
      job.externalJobId = startResult.jobId;
      this.saveQueue();
      
      // Poll for job completion
      await this.pollJobStatus(service, job);
      
      // Get the results
      if (job.status === 'completed') {
        await this.fetchAndStoreResults(service, job);
        
        // Publish job-completed event
        await this.adapter.publishJobCompleted({
          id: job.id,
          status: 'completed',
          priority: job.priority,
          url: job.config.url,
          provider: job.provider,
          progress: 100,
          result: job.result
        });
        
        // Transform results for training if requested
        if (job.config.transformForTraining) {
          await this.transformResultsForTraining(service, job);
        }
      }
    } catch (err) {
      // Handle job failure
      job.error = (err as Error).message;
      
      // Publish job-failed event
      await this.adapter.publishJobFailed({
        id: job.id,
        status: job.attempts < job.maxAttempts ? 'retrying' : 'failed',
        priority: job.priority,
        url: job.config.url,
        provider: job.provider,
        attempts: job.attempts,
        maxAttempts: job.maxAttempts,
        error: job.error
      });
      
      // Check if we should retry
      if (job.attempts < job.maxAttempts) {
        job.status = 'retrying';
        logger.info(`Will retry crawler job ${job.id} later (Attempt ${job.attempts}/${job.maxAttempts})`);
      } else {
        job.status = 'failed';
        job.completedAt = Date.now();
        logger.error(`Failed to process crawler job ${job.id} after ${job.attempts} attempts: ${err}`);
      }
      
      this.saveQueue();
    }
  }

  /**
   * Poll for job status until completion or failure
   * @param service Crawler service to use
   * @param job Job being processed
   */
  private async pollJobStatus(service: CrawlerService, job: CrawlerJob): Promise<void> {
    if (!job.externalJobId) {
      throw new Error('Job does not have an external job ID');
    }
    
    let attempts = 0;
    const maxAttempts = 60; // 30 minutes with 30s interval
    const pollInterval = 30000; // 30 seconds
    
    while (attempts < maxAttempts) {
      try {
        const status = await service.getJobStatus(job.externalJobId);
        
        job.progress = status.progress;
        this.saveQueue();
        
        // Publish job progress event
        await this.adapter.publishJobProgress({
          id: job.id,
          status: 'processing',
          priority: job.priority,
          url: job.config.url,
          provider: job.provider,
          progress: status.progress * 100
        });
        
        logger.debug(`Job ${job.id} status: ${status.status} (${Math.round(status.progress * 100)}%)`);
        
        if (status.status === 'completed') {
          job.status = 'completed';
          job.completedAt = Date.now();
          this.saveQueue();
          return;
        } else if (status.status === 'failed' || status.status === 'stopped') {
          job.status = 'failed';
          job.error = status.error || `Job ${status.status}`;
          job.completedAt = Date.now();
          this.saveQueue();
          return;
        }
        
        // Wait for the next poll
        await new Promise(resolve => setTimeout(resolve, pollInterval));
        attempts++;
      } catch (err) {
        logger.error(`Error polling job ${job.id} status: ${err}`);
        
        // Wait and retry
        await new Promise(resolve => setTimeout(resolve, pollInterval));
        attempts++;
      }
    }
    
    // Max attempts reached
    job.status = 'failed';
    job.error = 'Timed out waiting for job completion';
    job.completedAt = Date.now();
    this.saveQueue();
  }

  /**
   * Fetch and store crawler results
   * @param service Crawler service to use
   * @param job Completed job
   */
  private async fetchAndStoreResults(service: CrawlerService, job: CrawlerJob): Promise<void> {
    if (!job.externalJobId) {
      throw new Error('Job does not have an external job ID');
    }
    
    try {
      const result = await service.getJobResults(job.externalJobId);
      
      // Store results
      const resultPath = path.join(this.resultsDir, `${job.id}.json`);
      fs.writeFileSync(resultPath, JSON.stringify(result, null, 2));
      
      job.result = resultPath;
      this.saveQueue();
      
      logger.info(`Stored crawler results for job ${job.id}: ${resultPath}`);
    } catch (err) {
      logger.error(`Failed to fetch crawler results for job ${job.id}: ${err}`);
      throw err;
    }
  }

  /**
   * Transform crawler results to training data format
   * @param service Crawler service to use
   * @param job Completed job with results
   */
  private async transformResultsForTraining(service: CrawlerService, job: CrawlerJob): Promise<void> {
    if (!job.result) {
      throw new Error('Job does not have results');
    }
    
    try {
      // Load the stored results
      const resultData = JSON.parse(fs.readFileSync(job.result, 'utf-8')) as CrawlerResult;
      
      // Transform to training data
      const trainingDataDir = path.join(process.cwd(), 'data', 'training-data');
      fs.mkdirSync(trainingDataDir, { recursive: true });
      
      const transformResult = await service.transformResultsToTrainingData(
        resultData,
        trainingDataDir
      );
      
      job.trainingDataset = transformResult.datasetPath;
      job.status = 'training';
      this.saveQueue();
      
      logger.info(`Transformed crawler results to training data: ${transformResult.datasetPath}`);
      
      // Create a training job if requested
      if (job.config.autoTrain) {
        await this.startTrainingJob(job, transformResult.datasetPath);
      }
    } catch (err) {
      logger.error(`Failed to transform crawler results for job ${job.id}: ${err}`);
      throw err;
    }
  }

  /**
   * Start a model training job with crawler data
   * @param job Crawler job with transformed data
   * @param datasetPath Path to the training dataset
   */
  private async startTrainingJob(job: CrawlerJob, datasetPath: string): Promise<void> {
    try {
      // Import the ML functions
      const { trainModelWithCrawlerData } = await import('@kai/ml');
      
      // Generate a training job ID
      const trainingJobId = `crawler-train-${job.id}`;
      
      // Update training progress
      await trainingProgressService.updateProgress({
        jobId: trainingJobId,
        type: 'start',
        timestamp: Date.now(),
        data: {
          message: `Starting training with crawler data from ${job.config.url}`,
          modelType: job.config.trainingConfig?.modelType || 'hybrid',
          sourceJob: job.id,
          sourceType: 'crawler',
          dataProvenance: 'crawler'
        }
      });
      
      job.trainingJobId = trainingJobId;
      this.saveQueue();
      
      logger.info(`Started training job ${trainingJobId} for crawler job ${job.id}`);
      
      // Create the output directory for the trained model
      const outputDir = path.join(process.cwd(), 'data', 'models', trainingJobId);
      fs.mkdirSync(outputDir, { recursive: true });
      
      // Define the progress callback
      const progressCallback = (progress: number, message: string) => {
        trainingProgressService.updateProgress({
          jobId: trainingJobId,
          type: 'progress',
          timestamp: Date.now(),
          data: {
            message,
            progress,
            sourceJob: job.id,
            sourceType: 'crawler'
          }
        }).catch(err => {
          logger.error(`Failed to update training progress for job ${trainingJobId}: ${err}`);
        });
      };
      
      // Call the ML training function with crawler data
      trainModelWithCrawlerData({
        datasetPath,
        outputDir,
        modelType: job.config.trainingConfig?.modelType || 'hybrid',
        epochs: job.config.trainingConfig?.epochs || 10,
        batchSize: job.config.trainingConfig?.batchSize || 32,
        learningRate: job.config.trainingConfig?.learningRate || 0.001,
        progressCallback
      })
      .then(result => {
        // Update job with training results
        const updatedJob = this.jobs.get(job.id);
        if (updatedJob) {
          updatedJob.status = 'completed';
          this.saveQueue();
        }
        
        // Record the results in training progress
        trainingProgressService.updateProgress({
          jobId: trainingJobId,
          type: 'complete',
          timestamp: Date.now(),
          data: {
            message: 'Training completed successfully',
            accuracy: result.final_accuracy,
            modelPath: result.model_path,
            numClasses: result.num_classes,
            trainingTime: result.training_time,
            dataProvenance: 'crawler',
            sourceJob: job.id,
            sourceType: 'crawler'
          }
        }).catch(err => {
          logger.error(`Failed to update final training progress for job ${trainingJobId}: ${err}`);
        });
        
        logger.info(`Training job ${trainingJobId} completed successfully`);
      })
      .catch(error => {
        // Update job status
        const updatedJob = this.jobs.get(job.id);
        if (updatedJob) {
          updatedJob.status = 'failed';
          updatedJob.error = `Training failed: ${error.message}`;
          this.saveQueue();
        }
        
        // Update training progress with failure
        trainingProgressService.updateProgress({
          jobId: trainingJobId,
          type: 'error',
          timestamp: Date.now(),
          data: {
            message: `Training failed: ${error.message}`,
            error: error.message,
            sourceJob: job.id,
            sourceType: 'crawler'
          }
        }).catch(err => {
          logger.error(`Failed to update error training progress for job ${trainingJobId}: ${err}`);
        });
        
        logger.error(`Training job ${trainingJobId} failed: ${error}`);
      });
    } catch (err) {
      logger.error(`Failed to start training job for crawler job ${job.id}: ${err}`);
      throw err;
    }
  }
}

// Export singleton instance
export const crawlerQueue = new CrawlerQueue();