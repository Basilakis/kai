# KAI Platform Environment Configuration
# Centralized environment variables for all packages

# ============================================================
# OpenAI API Configuration
# ============================================================
# Your OpenAI API key (required)
OPENAI_API_KEY=your_openai_api_key_here
# Default model to use for agent operations
OPENAI_DEFAULT_MODEL=gpt-4
# Temperature setting for AI responses (0.0-1.0)
OPENAI_TEMPERATURE=0.7

# ============================================================
# Redis Configuration
# ============================================================
# Redis connection URL
REDIS_URL=redis://localhost:6379
# Redis server host
REDIS_HOST=localhost
# Redis server port
REDIS_PORT=6379
# Redis username if required
REDIS_USERNAME=
# Redis password if required
REDIS_PASSWORD=
# Redis database number
REDIS_DB=0

# ============================================================
# KAI Service Integration
# ============================================================
# Main KAI API URL
KAI_API_URL=http://localhost:3000/api
# Vector database service URL
KAI_VECTOR_DB_URL=http://localhost:5000/api/vector
# Machine learning service URL
KAI_ML_SERVICE_URL=http://localhost:7000/api/ml
# API Key for KAI service authentication
KAI_API_KEY=kai_dev_key
# API URL (if different from KAI_API_URL)
API_URL=http://localhost:3000/api

# ============================================================
# Logging Configuration
# ============================================================
# Log level (error, warn, info, debug, verbose, silly)
LOG_LEVEL=info
# Path to log file
LOG_FILE_PATH=logs/agent.log
# Whether to output logs to console (true/false)
LOG_CONSOLE_OUTPUT=true

# ============================================================
# Agent Behavior Configuration
# ============================================================
# Enable verbose mode for agents
AGENT_VERBOSE_MODE=false
# Enable agent memory persistence
AGENT_MEMORY_ENABLED=true
# Maximum number of iterations for agent tasks
AGENT_MAX_ITERATIONS=10
# Default timeout for agent operations in milliseconds
AGENT_TIMEOUT=30000
# Maximum number of concurrent agent sessions
MAX_CONCURRENT_SESSIONS=10
# API key for agent authentication
AGENT_API_KEY=development-key
# Enable mock services as fallback when real services are unavailable
ENABLE_MOCK_FALLBACK=true

# ============================================================
# Server Configuration
# ============================================================
# Port for server to listen on
PORT=3000
# Node environment (development, production, test)
NODE_ENV=development
# JWT secret for authentication
JWT_SECRET=your_jwt_secret_here_for_dev
# JWT token expiration time (in seconds)
JWT_EXPIRATION=86400

# ============================================================
# AWS/S3 Storage Configuration
# ============================================================
# AWS region
AWS_REGION=us-east-1
# AWS access key ID
AWS_ACCESS_KEY_ID=
# AWS secret access key
AWS_SECRET_ACCESS_KEY=
# S3 bucket name
S3_BUCKET=kai-storage
# Temporary directory for file processing
TEMP=/tmp

# ============================================================
# Supabase Configuration
# ============================================================
# Supabase URL
SUPABASE_URL=
# Supabase API key
SUPABASE_KEY=
# Supabase storage bucket
SUPABASE_STORAGE_BUCKET=materials
# Supabase URL for Gatsby client (must be prefixed with GATSBY_)
GATSBY_SUPABASE_URL=
# Supabase anonymous key for Gatsby client
GATSBY_SUPABASE_ANON_KEY=

# ============================================================
# HuggingFace Configuration
# ============================================================
# HuggingFace API key
HF_API_KEY=
# HuggingFace organization ID
HF_ORGANIZATION_ID=
# Default dataset provider (e.g., 'huggingface')
DEFAULT_DATASET_PROVIDER=

# ============================================================
# MCP (Machine Learning Control Plane) Configuration
# ============================================================
# MCP server URL
MCP_SERVER_URL=http://localhost:8000
# Whether to use MCP server
USE_MCP_SERVER=false
# Timeout for MCP server health check in milliseconds
MCP_HEALTH_CHECK_TIMEOUT=5000
# Path to model files
MODEL_PATH=/app/models
# Number of models to keep in memory
MODEL_CACHE_SIZE=5
# Enable GPU acceleration if hardware is available
GPU_ENABLED=true
# Enable agent integration with MCP
AGENT_INTEGRATION_ENABLED=true
# Maximum batch size for inference
MAX_BATCH_SIZE=16
# Timeout for model loading (ms)
MODEL_LOAD_TIMEOUT=30000
# Enable model quantization for optimization
ENABLE_QUANTIZATION=false
# Number of worker processes
WORKER_PROCESSES=4
# Maximum number of messages in agent queue
MAX_AGENT_QUEUE_SIZE=100
# Agent message timeout (ms)
AGENT_MESSAGE_TIMEOUT=5000
# Enable agent feedback loop
AGENT_FEEDBACK_ENABLED=true

# ============================================================
# Gatsby/Client Configuration
# ============================================================
# API URL for frontend to connect to backend services
GATSBY_API_URL=http://localhost:3000/api
# WebSocket URL for real-time communication
GATSBY_WS_URL=ws://localhost:3000
# Enable/disable features in development
GATSBY_ENABLE_RECOGNITION_FEATURE=true
GATSBY_ENABLE_AGENT_CHAT=true
GATSBY_ENABLE_OFFLINE_MODE=true
# Analytics tracking ID (if any)
GATSBY_ANALYTICS_ID=
# Application version
REACT_APP_VERSION=1.0.0
# Application name
GATSBY_APP_NAME=KAI Platform
# Increase Node memory for builds if needed
NODE_OPTIONS=--max-old-space-size=4096
# Control image processing during build
GATSBY_CONCURRENT_DOWNLOAD=5
# Control number of workers for parallel processing
PARALLEL_QUERY_RUNNING_JOBS=4